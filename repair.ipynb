{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Program Repair"
      ],
      "metadata": {
        "id": "NaiEJ7s62dER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zppwvue-cBuL"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\" # this is needed to get rid of weird colab locale error\n",
        "# if you are still running into issues, please restart the runtime to initialize a new environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the accelerate library\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "xeZRUs4nwlV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/evalplus/evalplus\n",
        "!pip install evalplus==0.2.0"
      ],
      "metadata": {
        "id": "av2GdKYZ9Mp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/uiuc-cs598lmz-s24/hw3/main/buggy_humaneval.jsonl"
      ],
      "metadata": {
        "id": "GWCzGJ7bah17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def grab_buggy_dataset():\n",
        "    inference_dataset = []\n",
        "    file = \"buggy_humaneval.jsonl\"\n",
        "    with open(file, \"r\") as f:\n",
        "        inference_dataset.extend([json.loads(x) for x in f.readlines()])\n",
        "    print(\"Number of tasks: {}\".format(len(inference_dataset)))\n",
        "    return inference_dataset\n",
        "\n",
        "buggy_humaneval = grab_buggy_dataset()\n",
        "# feel free to play around the dataset for a bit"
      ],
      "metadata": {
        "id": "g8js29TPCz22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration\n",
        "\n",
        "def load_codegen_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\", device_map='auto', torch_dtype=torch.float16)\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_codet5_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base\", device_map='auto')\n",
        "    return model, tokenizer\n",
        "\n",
        "codegen, codegen_tokenizer = load_codegen_model()\n",
        "codet5, codet5_tokenizer = load_codet5_model()"
      ],
      "metadata": {
        "id": "WrdVC1gA1tTt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/evalplus/evalplus\n",
        "!pip install evalplus==0.2.0"
      ],
      "metadata": {
        "id": "y5QGDzObv6OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the folder to save the results\n",
        "!mkdir codegen_fix"
      ],
      "metadata": {
        "id": "qV2Er_0IzwVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def codegen_program_repair(buggy_program: str, buggy_line_index: int, model, tokenizer) -> str:\n",
        "    # TODO: implement greedy solution using codegen.\n",
        "    # note, you want to return a complete function here\n",
        "    # note, there might be some post processing needed to remove irrelevant\n",
        "    # tokens.\n",
        "    # you can access the buggy line using 'buggy_program.split(\"\\n\")[buggy_line_index]'\n",
        "    return buggy_program\n",
        "\n",
        "\n",
        "def repair_humaneval_codegen(model, tokenizer, buggy_humaneval, workdir):\n",
        "  for bug in tqdm(buggy_humaneval):\n",
        "      name = bug['task_id'].replace(\"/\", \"_\")\n",
        "      buggy_code = bug['buggy_code']\n",
        "      buggy_line_index = bug['buggy_line']\n",
        "\n",
        "      fixed_code = codegen_program_repair(buggy_code, buggy_line_index, model, tokenizer)\n",
        "      os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
        "      with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
        "          f.write(fixed_code)\n",
        "\n",
        "\n",
        "# generate the solutions produced by codegen\n",
        "repair_humaneval_codegen(codegen, codegen_tokenizer, buggy_humaneval, \"codegen_fix\")"
      ],
      "metadata": {
        "id": "RUHkSvn20caK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now place take a look at the solutions produced by codegen in the folder\n",
        "# we will now evaluate the solution\n",
        "# note you can passing in \"--i-just-wanna-run\" to this command to\n",
        "# recompute the results IF and ONLY IF you have made some updates to each solution file :)\n",
        "!yes Y | evalplus.evaluate --dataset humaneval --samples codegen_fix --i-just-wanna-run"
      ],
      "metadata": {
        "id": "Wl9I1P1W3JNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the folder to save the results\n",
        "!mkdir codet5_fix"
      ],
      "metadata": {
        "id": "ugXgLw9mH50O"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def codet5_program_repair(buggy_program: str, buggy_line_index: int, model, tokenizer) -> str:\n",
        "    # TODO: implement greedy solution using codet5.\n",
        "    # note, you want to return a complete function here\n",
        "    # note, there might be some post processing needed to remove irrelevant\n",
        "    # tokens.\n",
        "    # you can access the buggy line using 'buggy_program.split(\"\\n\")[buggy_line_index]'\n",
        "    return buggy_program\n",
        "\n",
        "\n",
        "def repair_humaneval_codet5(model, tokenizer, buggy_humaneval, workdir):\n",
        "  for bug in tqdm(buggy_humaneval):\n",
        "      name = bug['task_id'].replace(\"/\", \"_\")\n",
        "      buggy_code = bug['buggy_code']\n",
        "      buggy_line_index = bug['buggy_line']\n",
        "\n",
        "      fixed_code = codet5_program_repair(buggy_code, buggy_line_index, model, tokenizer)\n",
        "      os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
        "      with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
        "          f.write(fixed_code)\n",
        "\n",
        "\n",
        "repair_humaneval_codet5(codet5, codet5_tokenizer, buggy_humaneval, \"codet5_fix\")"
      ],
      "metadata": {
        "id": "ja4N2kOcHvFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now place take a look at the solutions produced by codegen in the folder\n",
        "# we will now evaluate the solution\n",
        "# note you can passing in \"--i-just-wanna-run\" to this command to\n",
        "# recompute the results IF and ONLY IF you have made some updates to each solution file :)\n",
        "!yes Y | evalplus.evaluate --dataset humaneval --samples codet5_fix --i-just-wanna-run"
      ],
      "metadata": {
        "id": "saLO9GtLH-SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evalplus.data import get_human_eval_plus\n",
        "\n",
        "def check_which_passed(workdir: str, dataset):\n",
        "    with open(os.path.join(workdir, \"eval_results.json\"), \"r\") as f:\n",
        "        results = json.loads(f.read())\n",
        "\n",
        "    failed_humaneval = []\n",
        "    failed_humaneval_plus = []\n",
        "\n",
        "    for task_id in dataset.keys():\n",
        "        total = results['eval'][task_id]['nfiles']\n",
        "        humaneval_base = len([x for x in results['eval'][task_id]['base'] if x[0] == \"success\"]) / total\n",
        "        humaneval_plus = len([x for x in results['eval'][task_id]['plus'] if x[0] == \"success\"]) / total\n",
        "\n",
        "        if humaneval_base == 1:\n",
        "            failed_humaneval.append(task_id)\n",
        "        if humaneval_plus == 1:\n",
        "            failed_humaneval_plus.append(task_id)\n",
        "\n",
        "    return failed_humaneval, failed_humaneval_plus\n"
      ],
      "metadata": {
        "id": "lw70jZp98NGE"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use this to check which problem the model fixed\n",
        "success_humaneval, success_humaneval_plus = check_which_passed(\"codegen_fix\", get_human_eval_plus())"
      ],
      "metadata": {
        "id": "ZH5o1fgV9Ng4"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "success_humaneval_codet5, success_humaneval_plus_codet5 = check_which_passed(\"codet5_fix\", get_human_eval_plus())"
      ],
      "metadata": {
        "id": "UBXTt304IOKH"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Template-based Program Repair"
      ],
      "metadata": {
        "id": "2PAyVz_rIUOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make the folder to save the results\n",
        "!cp -r codet5_fix codet5_fix_template"
      ],
      "metadata": {
        "id": "dOVHXe_ue59n"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def template_codet5_program_repair(buggy_program: str, buggy_line_index: int, model, tokenizer) -> str:\n",
        "    # TODO: implement greedy solution using codet5.\n",
        "    # note, you want to return a complete function here\n",
        "    # note, there might be some post processing needed to remove irrelevant\n",
        "    # tokens.\n",
        "    # you can access the buggy line using 'buggy_program.split(\"\\n\")[buggy_line_index]'\n",
        "    return buggy_program\n",
        "\n",
        "\n",
        "def repair_humaneval_codet5_template(model, tokenizer, buggy_humaneval, fixed_bugs, workdir):\n",
        "  for bug in tqdm(buggy_humaneval):\n",
        "      name = bug['task_id'].replace(\"/\", \"_\")\n",
        "      if bug['task_id'] in fixed_bugs:\n",
        "        continue\n",
        "      buggy_code = bug['buggy_code']\n",
        "      buggy_line_index = bug['buggy_line']\n",
        "\n",
        "      fixed_code = template_codet5_program_repair(buggy_code, buggy_line_index, model, tokenizer)\n",
        "      os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
        "      with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
        "          f.write(fixed_code)\n",
        "\n",
        "# generate the solutions produced by codegen\n",
        "repair_humaneval_codet5_template(codet5, codet5_tokenizer, buggy_humaneval, success_humaneval_codet5, \"codet5_fix_template\")"
      ],
      "metadata": {
        "id": "rAN0kxUjtSNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note you can passing in \"--i-just-wanna-run\" to this command to\n",
        "# recompute the results IF and ONLY IF you have made some updates to each solution file :)\n",
        "# you may need to pass in (yes Y | command) on colab\n",
        "!yes Y | evalplus.evaluate --dataset humaneval --samples codet5_fix_template --i-just-wanna-run"
      ],
      "metadata": {
        "id": "MfM9aLs741F_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}